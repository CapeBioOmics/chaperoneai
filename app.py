from flask import Flask, render_template, request, redirect, url_for, send_from_directory, flash
from werkzeug.utils import secure_filename
import os
import subprocess
import pandas as pd
import joblib


app = Flask(__name__)

# Configuration
app.config['UPLOAD_FOLDER'] = 'uploads'
app.config['OUTPUT_FASTQC'] = 'outputs/output_fastqc'
app.config['OUTPUT_VELVET'] = 'outputs/output_velvet'
app.config['OUTPUT_PRODIGAL'] = 'outputs/output_prodigal'
app.config['OUTPUT_ECPRED'] = 'outputs/output_ecpred'
app.config['ALLOWED_EXTENSIONS'] = {'fastq', 'gz','fa', 'fasta', 'faa', 'fa', 'fq'}

# Ensure the upload and output directories exist
for key in app.config:
    if key.startswith('OUTPUT_') and not os.path.exists(app.config[key]):
        os.makedirs(app.config[key])

# Function to check if a file has an allowed extension
def allowed_file(filename):
    return '.' in filename and filename.rsplit('.', 1)[1].lower() in app.config['ALLOWED_EXTENSIONS']

@app.route('/')
def base():
    return render_template('base.html')


#-------quality check--------=============================================================================##
@app.route('/check_quality', methods=['GET', 'POST'])
def check_quality():
    if request.method == 'POST':
        file = request.files['file']
        if file.filename == '':
            return redirect(request.url)

        filename = os.path.join(app.config['UPLOAD_FOLDER'], secure_filename(file.filename))
        file.save(filename)

        cmd = ['./FastQC/fastqc', filename, '-o', 'outputs/output_fastqc']
        subprocess.run(cmd)

        # Extract the name of the HTML report generated by FastQC
        base_filename, _ = os.path.splitext(secure_filename(file.filename))
        html_report = f"{base_filename}_fastqc.html"
        zip_report = f"{base_filename}_fastqc.zip"

        return render_template('quality_check.html', uploaded=True, html_report=html_report, zip_report=zip_report)

    return render_template('quality_check.html', uploaded=False)

# Add a new route to serve the HTML report
@app.route('/outputs/output_fastqc/<html_report>')
def view_html_report(html_report):
    html_path = os.path.join(app.root_path, 'outputs', 'output_fastqc', html_report)
    if os.path.exists(html_path):
        return send_from_directory(os.path.join(app.root_path, 'outputs', 'output_fastqc'), html_report)
    else:
        return f"HTML report '{html_report}' not found."
    


#-------Protein Visualization--------=============================================================================##
    
import nglview as nv
from Bio.PDB import PDBParser
import gzip
import shutil

def return_3D_structure_with_highlights(pdb_id, select_chain, res_to_highlight, len_seq):
    path = "protein_data/"
    input_gz_file = path + pdb_id + ".ent.gz"
    output_file = path + pdb_id + ".ent"

    # Decompress the .ent.gz file
    with gzip.open(input_gz_file, 'rb') as f_in:
        with open(output_file, 'wb') as f_out:
            shutil.copyfileobj(f_in, f_out)

    # Load the structure directly from the decompressed .ent file
    parser = PDBParser(QUIET=True)
    structure = parser.get_structure("Test", output_file)

    assert len(structure) == 1
    chain_selected = structure[0][select_chain]

    all_res = list(chain_selected.get_residues())
    nb_res = len(all_res)

    atoms_to_highlight = []
    atoms_to_not_highlight = []

    print("There are", nb_res, "residues")
    
    view = nv.show_biopython(chain_selected)
    num_atom = 0

    for indice, res in enumerate(chain_selected.get_residues()):
        if indice >= len_seq:
            break
        list_atoms = list(res.get_atoms())
        for _ in range(len(list_atoms)):
            if indice in res_to_highlight:
                atoms_to_highlight.append(num_atom)
            else:
                atoms_to_not_highlight.append(num_atom)
            num_atom += 1

    # Clear previous representations and add new ones
    view.clear_representations()
    view.add_representation('spacefill', selection=atoms_to_not_highlight, color="white")
    view.add_representation('spacefill', selection=atoms_to_highlight, color="blue")
    
    return view


@app.route('/protein_visualization', methods=['GET', 'POST'])
def protein_visualization():
    # Example usage:
    pdb_id = "pdb4e09"
    select_chain = "A"  # Change to the desired chain
    res_to_highlight = [1, 5, 10, 15, 20]  # Change to the desired residue indices
    len_seq = 2000  # Change to the desired length


    view = return_3D_structure_with_highlights(pdb_id, select_chain, res_to_highlight, len_seq)
    
    # Convert NGLView widget to HTML
    #view_html = view._repr_html_()
    view_html = view.render_template(view)

    return render_template('protein_visualization.html', view_html=view_html, uploaded=False)



#-------Genome Assembly--------=============================================================================##
@app.route('/genome_assembly', methods=['GET', 'POST'])
def genome_assembly():
    feedback = None
    contigs = None
    stats = None
    OUTPUT_VELVET = 'outputs/output_velvet'

    if request.method == 'POST':
        files = request.files.getlist('file')

        if len(files) != 2:
            feedback = "Please upload exactly two files for paired-end reads."
            return render_template('genome_assembly.html', feedback=feedback)

        file_paths = []
        for file in files:
            if file and allowed_file(file.filename):
                filename = secure_filename(file.filename)
                filepath = os.path.join(app.config['UPLOAD_FOLDER'], filename)
                file.save(filepath)
                file_paths.append(filepath)

        hash_length = 31
        cmd_velveth = f"velveth {OUTPUT_VELVET} {hash_length} -shortPaired -separate -fastq {' '.join(file_paths)}"
        cmd_velvetg = f"velvetg {OUTPUT_VELVET}"

        os.system(cmd_velveth)
        os.system(cmd_velvetg)

        # Check if contigs.fa and stats.txt exist
        if os.path.exists(os.path.join(OUTPUT_VELVET, 'contigs.fa')):
            contigs = 'contigs.fa'
        if os.path.exists(os.path.join(OUTPUT_VELVET, 'stats.txt')):
            stats = 'stats.txt'

        feedback = "Upload and assembly successful!"

    return render_template('genome_assembly.html', feedback=feedback, contigs=contigs, stats=stats)


#-------Protein Seq Prodigal--------=============================================================================##

# Ensure the upload and output directories exist
for key in app.config:
    if key.startswith('OUTPUT_') and not os.path.exists(app.config[key]):
        os.makedirs(app.config[key])

# Function to check if a file has an allowed extension
def allowed_file(filename):
    return '.' in filename and filename.rsplit('.', 1)[1].lower() in app.config['ALLOWED_EXTENSIONS']

# Function to run Prodigal and save protein sequences
def Protein_Sequence_Prediction(input_path, output_path):
    try:
        cmd = f'prodigal -i {input_path} -a {output_path}'
        subprocess.run(cmd, check=True, capture_output=True, shell=True)
        return True, "Protein sequences generated successfully"
    except subprocess.CalledProcessError as e:
        return False, f"Error in gene prediction: {e}"
    


@app.route('/protein_sequence_prediction', methods=['POST', 'GET'])
def protein_sequence_prediction():
    feedback = None
    protein_file = None

    if request.method == 'POST':
        file = request.files['file']
        if file.filename == '':
            return redirect(request.url)

        if file and allowed_file(file.filename):
            filename = secure_filename(file.filename)
            filepath = os.path.join(app.config['UPLOAD_FOLDER'], filename)
            file.save(filepath)

            output_path = os.path.join(app.config['OUTPUT_PRODIGAL'], 'out.faa')
            success, feedback = Protein_Sequence_Prediction(filepath, output_path)
            protein_file = 'out.faa' if success else None

            # Optionally, clean up the uploaded file after processing
            os.remove(filepath)

        else:
            feedback = "Invalid file type. Please upload a .fa or .fasta file."

    return render_template('protein_sequence_prediction.html', feedback=feedback, protein_file=protein_file)




############ECPred and KofmanScan################
#read from output of prodigal test.faa
#test.faa is the input to ECPred, and ECPred produces test.tsv(labeled nodes and EC nunmbers)
#Preprocess test.tsv and test.faa to produce final dataframe for ML

@app.route('/sequence_labeling')
def sequence_labeling():
    return render_template('sequence_labeling.html')

def run_ecpred(input_fasta, output_file, method='blast'):
    ecpred_path = '/Users/jamesjr/Documents/Genomics/chaperone/backend_free_version/ECPred'
    command = f"java -jar {ecpred_path}/ECPred.jar {method} {input_fasta} {ecpred_path}/ {ecpred_path}/temp {output_file}"
    
    try:
        result = subprocess.run(command, shell=True, check=True, capture_output=True, text=True)
        if result.returncode == 0:
            return f"ECPred completed successfully. Results saved to {output_file}"
        else:
            error_message = f"ECPred encountered an error. Return code: {result.returncode}\n{result.stderr}"
            raise subprocess.CalledProcessError(returncode=result.returncode, cmd=command, output=result.stdout, stderr=error_message)
    except subprocess.CalledProcessError as e:
        return f"ECPred execution failed: {e}\n{e.stderr}"

# Other functions and routes remain the same

def extract_sequence_info(sequence):
    info = {}
    info['Node_ID'] = re.search(r'>(NODE_\d+_length_\d+_cov_\d+\.\d+_\d)', sequence).group(1)
    info['Length'] = re.search(r'length_(\d+)', sequence).group(1)
    info['Coverage'] = re.search(r'cov_(\d+\.\d+)', sequence).group(1)
    info['Node_Number'] = re.search(r'_(\d+)\s', sequence).group(1)
    
    protein_sequence = sequence.split(';')[-1] if ';' in sequence else sequence
    gc_match = re.search(r'gc_cont=(\d+\.\d+)([A-Z]+)', protein_sequence)
    info['GC_count'] = gc_match.group(1) if gc_match else None
    info['Protein_Sequence'] = gc_match.group(2) if gc_match else protein_sequence
    
    return pd.Series(info)

def label_ec_number(ec_number):
    if ec_number == 'non Enzyme' or ec_number == 'no Prediction':
        return '0'
    else:
        return ec_number[0]

def extract_and_split_amino_acids(protein_sequence):
    amino_acids = ''.join([char for char in protein_sequence if char.isalpha() or char.isspace()])
    return ' '.join([amino_acids[i:i+3] for i in range(0, len(amino_acids), 3)])


import logging

# Add this at the beginning of your script to configure logging
logging.basicConfig(level=logging.DEBUG)


@app.route('/run_ecpred_route', methods=['POST', 'GET'])
def run_ecpred_route():
    feedback = None
    output_file = None
    if 'file' in request.files:
        uploaded_file = request.files['file']

        if uploaded_file.filename == '':
            feedback = "No file selected."
        elif uploaded_file and uploaded_file.filename.endswith(('.fa', '.fasta', '.faa')):
            filename = secure_filename(uploaded_file.filename)
            filepath = os.path.join(app.config['UPLOAD_FOLDER'], filename)
            uploaded_file.save(filepath)

            logging.info(f"Filepath before run_ecpred: {filepath}")
            output_path = os.path.join(app.config['OUTPUT_ECPRED'], 'output_proteins.tsv')
            feedback = run_ecpred(filepath, output_path)
            logging.info(f"Feedback from run_ecpred: {feedback}")

            output_file = 'output_proteins.tsv' if 'successfully' in feedback else None
            logging.info(f"Output path: {output_path}")
            logging.info(f"Files in output directory: {os.listdir(os.path.dirname(output_path))}")

            os.remove(filepath)
        else:
            feedback = "Invalid file type. Please upload a .fa, .fasta, or .faa file."

    return render_template('run_ecpred.html', feedback=feedback, output_file=output_file)

@app.route('/outputs/<path:subdir>/<filename>')
def output_files(subdir, filename):
    return send_from_directory(os.path.join('outputs', subdir), filename, as_attachment=False)


@app.route('/kofmanscan_route')
def kofmanscan_route():
    #----------------
    return render_template('kofmanscan_route.html')






#########Machine Learning Starts Here!!!!

from sklearn.feature_extraction.text import TfidfVectorizer
from yellowbrick.text import TSNEVisualizer
from yellowbrick.datasets import load_hobbies
from collections import Counter
from sklearn.datasets import make_classification
from imblearn.over_sampling import SMOTE 
from sklearn.model_selection import StratifiedKFold
from flask import request
import pandas as pd
from sklearn.model_selection import cross_val_predict
from flask import request
#import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from imblearn.over_sampling import SMOTE
from sklearn.model_selection import StratifiedKFold, cross_val_score
from collections import Counter
import pickle
import numpy as np
from flask import json


@app.route('/ml_preprocessing', methods=['POST', 'GET'])
def ml_preprocessing():
    return render_template('ml_processing_temp.html')

#----------------------------------------------------+++++++++++++++++++++

@app.route('/enzyme_ec_predictions')
def enzyme_ec_predictions():
    return render_template('enzyme_ec_prediction.html')
#-------------


#import os
class_mapping = {
    0: 'ligases',
    1: 'lyases',
    2: 'hydrolases',
    3: 'isomerase',
    4: 'translocase',
    5: 'transferases',
    6: 'non enzyme',
    7: 'oxidoreductase'
}

# Load the TF-IDF vectorizer trained on the original data
# Assuming tfidf is the object used during training
#with open('trained_embeddings/tfidf_vectorizer.pkl', 'rb') as f:
#    tfidf = pickle.load(f)

# ... Your existing imports ...
# Initialize the TF-IDF vectorizer outside the route function
#tfidf = TfidfVectorizer()
# ... (other imports)
from sklearn.feature_extraction.text import TfidfVectorizer
from imblearn.over_sampling import SMOTE
from sklearn.model_selection import StratifiedKFold, cross_val_score
import pickle
import numpy as np
from sklearn.preprocessing import LabelEncoder

# ... (other route definitions)

@app.route('/analyze_enzyme_csv', methods=['POST'])
def analyze_enzyme_csv():
    # Check if a file was uploaded in the request
    if 'file' not in request.files or not request.files['file'].filename:
        result = {"error": "No file selected"}
        print(result)  # Print to terminal
        return render_template('enzyme_ec_prediction.html', result=result), 400

    file = request.files['file']

    # Check if the file has an allowed extension (e.g., CSV)
    if not file.filename.endswith('.csv'):
        result = {"error": "Invalid file type. Please upload a CSV file"}
        print(result)  # Print to terminal
        return render_template('enzyme_ec_prediction.html', result=result), 400

    try:
        # Read the CSV file into a Pandas DataFrame
        df = pd.read_csv(file)
        df = df.dropna()

        # Check if the DataFrame has labels ('label' column)
        if 'label' in df.columns:
            X = df.words
            y = df.label

            # Load the data and create document vectors
            tfidf = TfidfVectorizer()
            X_t = tfidf.fit_transform(X)

            # Evaluate the model using cross-validation
            kf = StratifiedKFold(n_splits=3)
            loaded_model = pickle.load(open('models/sgd_tf_idf_multiclass_SMOTE.sav', 'rb'))

            # Metrics
            acc_scores = cross_val_score(loaded_model, X_t, y, cv=kf, scoring="accuracy")
            f1_scores = cross_val_score(loaded_model, X_t, y, cv=kf, scoring="f1_weighted")
            recall_scores = cross_val_score(loaded_model, X_t, y, cv=kf, scoring="recall_weighted")
            precision_scores = cross_val_score(loaded_model, X_t, y, cv=kf, scoring="precision_weighted")

            result = {
                "message": "Analysis completed",
                "mean_acc_score": np.mean(acc_scores) * 100,
                "mean_f1_score": np.mean(f1_scores) * 100,
                "mean_recall_score": np.mean(recall_scores) * 100,
                "mean_precision_score": np.mean(precision_scores) * 100
            }

            # Print results to the terminal
            print(result)

            # Save results to a text file
            result_text = f"Analysis Results:\n{result}"
            with open('ML_results/enzyme_results.txt', 'w') as file:
                file.write(result_text)

            predictions = loaded_model.predict(X_t)

            # Map predicted labels back to class names
            predictions_class_mapped = [class_mapping[label] for label in predictions]

            y_class_mapped = [class_mapping[label] for label in y]


            # Display the results
            result_df = pd.DataFrame({'words': X, 'true_labels': y_class_mapped, 'predicted_labels': predictions_class_mapped})

            # Save the result DataFrame to a CSV file
            result_df.to_csv('ML_results/predictions_result_with_true_labels.csv', index=False)

            

        else:
            # If labels are not present, print predictions using the only column as input
            loaded_model = pickle.load(open('models/sgd_tf_idf_multiclass_NO_SMOTE.sav', 'rb'))
            with open('trained_embeddings/tfidf_vectorizer.pkl', 'rb') as f:
                tfidf = pickle.load(f)

            try:
                # Read the CSV file into a Pandas DataFrame
                if 'words' in df.columns and len(df.columns) == 1:
                    X_unlabeled = df['words']

                    # TF-IDF vectorization
                    X_unlabeled_t = tfidf.transform(X_unlabeled)

                    # Load the model and make predictions
                    #loaded_model = pickle.load(open('models/sgd_tf_idf_multiclass_NO_SMOTE.sav', 'rb'))
                    predictions = loaded_model.predict(X_unlabeled_t)

                    # Map predicted labels back to class names
                    predictions_class_mapped = [class_mapping[label] for label in predictions]

                    # Display the results
                    result_df = pd.DataFrame({'words': X_unlabeled, 'predicted_class': predictions_class_mapped})

                    # Save the result DataFrame to a CSV file
                    result_df.to_csv('ML_results/nolabels_predictions_result.csv', index=False)

                    # Display the results
                    result = {"message": "Model predictions saved to 'predictions_result.csv'", "predictions": predictions}

                    return render_template('enzyme_ec_prediction.html', result=result)

                else:
                    result = {"error": "Invalid CSV format. The file should have only one column named 'words'"}
                    return render_template('enzyme_ec_prediction.html', result=result), 400

            except Exception as e:
                result = {"error": f"Error processing CSV file: {str(e)}"}
                return render_template('enzyme_ec_prediction.html', result=result), 500

        return render_template('enzyme_ec_prediction.html', result=result)

    except Exception as e:
        result = {"error": f"Error processing CSV file: {str(e)}"}
        print(result)  # Print to terminal
        return render_template('enzyme_ec_prediction.html', result=result), 500



####---------------########---------------
############----------------------##########

def getKmers(sequence, size=3):
    return [sequence[x:x+size].lower() for x in range(len(sequence) - size + 1)]

@app.route('/analyze_enzyme_sequence', methods=['POST'])
def analyze_enzyme_sequence():
    # Get the sequence from the form data
    sequence = request.form.get('sequence')

    # Check if a sequence was provided
    if not sequence:
        result = {"error": "No sequence provided"}
        return render_template('enzyme_ec_prediction.html', result=result)

    # Use the trained model to predict the class
    try:
        # Preprocess the sequence (you might need to adapt this based on your preprocessing steps during training)
        kmer_size = 3  # or use any other desired k-mer size
        preprocessed_sequence = getKmers(sequence, size=kmer_size)

        # Use the loaded model to make predictions
        loaded_model = joblib.load(open('models/sgd_tf_idf_multiclass.sav', 'rb'))
        # Assuming your model takes a preprocessed sequence as input
        prediction = loaded_model.predict([preprocessed_sequence])[0]
        print(prediction)

        # Map the predicted label back to the class name
        predicted_class = class_mapping[prediction]

        result = {"message": "Sequence analysis result", "predicted_class": predicted_class}
        # Introduce a delay of 30 seconds
        time.sleep(30)
        return render_template('enzyme_ec_prediction.html', result=result)

    except Exception as e:
        result = {"error": f"Error during sequence analysis: {str(e)}"}
        return render_template('enzyme_ec_prediction.html', result=result)

#----------------------------------------------------+++++++++++++++++++++

@app.route('/peptide_activity_predictions')
def peptide_activity_prediction():
    return render_template('peptide_activity_prediction.html')

@app.route('/analyze_peptide_csv', methods=['POST'])
def analyze_peptide_csv():
    # Implement CSV analysis logic here
    result = {"message": "CSV file analysis result"}
    return render_template('peptide_activity_prediction.html', result=result)

#@app.route('/analyze_peptide_sequence', methods=['POST'])
#def analyze_peptide_sequence():
#    # Implement sequence analysis logic here
#    result = {"message": "Sequence analysis result"}
#    return render_template('peptide_activity_prediction.html', result=result)

#import os
#path = '/Users/zhenjiaodu/no_icloud/1. ksu-in class/CIS 730 introduction to artifical intelligence/term project/BERT_4_ACE/uber_rides_prediction_using_machine_learning'
#os.chdir(path)

import numpy as np
from flask import Flask, request, jsonify, render_template
import pickle
import math
import collections
from keras.models import load_model
from flask import Flask,request, url_for, redirect, render_template
from sklearn.preprocessing import MinMaxScaler
import pandas as pd

#app = Flask(__name__)
# model = pickle.load(open('model.pkl','rb'))
# load the prediction model
peptide_model = load_model('peptide_models_and_data/DPPIV_tensorflow_model')
# embeddings function
def esm_embeddings(peptide_sequence_list: list):
    # NOTICE: ESM for embeddings is quite RAM usage, if your sequence is too long,
    #         or you have too many sequences for transformation in a single converting,
    #         you conputer might automatically kill the job.
    # return a panda.dataframe
    import torch
    import pandas as pd
    import esm
    import collections
    # load the model
    # NOTICE: if the model was not downloaded in your local environment, it will automatically download it.
    peptide_model, alphabet = esm.pretrained.esm2_t6_8M_UR50D()
    batch_converter = alphabet.get_batch_converter()
    peptide_model.eval()  # disables dropout for deterministic results

    # load the peptide sequence list into the bach_converter
    batch_labels, batch_strs, batch_tokens = batch_converter(peptide_sequence_list)
    batch_lens = (batch_tokens != alphabet.padding_idx).sum(1)
    ## batch tokens are the embedding results of the whole data set

    # Extract per-residue representations (on CPU)
    with torch.no_grad():
        # Here we export the last layer of the EMS model output as the representation of the peptides
       # model'esm2_t6_8M_UR50D' only has 6 layers, and therefore repr_layers parameters is equal to 6
        results = model(batch_tokens, repr_layers=[6], return_contacts=True)
    token_representations = results["representations"][6]

    # Generate per-sequence representations via averaging
    # NOTE: token 0 is always a beginning-of-sequence token, so the first residue is token 1.
    sequence_representations = []
    for i, tokens_len in enumerate(batch_lens):
        sequence_representations.append(token_representations[i, 1 : tokens_len - 1].mean(0))
    # save dataset
    # sequence_representations is a list and each element is a tensor
    embeddings_results = collections.defaultdict(list)
    for i in range(len(sequence_representations)):
        # tensor can be transformed as numpy sequence_representations[0].numpy() or  sequence_representations[0].to_list
        each_seq_rep = sequence_representations[i].tolist()
        for each_element in each_seq_rep:
            embeddings_results[i].append(each_element)
    embeddings_results = pd.DataFrame(embeddings_results).T
    return embeddings_results

# normalized the embeddings
X_train_data_name = 'peptide_models_and_data/DPPIV_train_esm2_t6_8M_UR50D_unified_320_dimension.csv'
X_train_data = pd.read_csv(X_train_data_name, header=0, index_col=0, delimiter=',')
X_train = np.array(X_train_data)
# normalize the X data range
scaler = MinMaxScaler()
scaler.fit(X_train)
# scaler.transform will automatically transform the pd.dataframe into a np.array data format

# collect the output
def assign_activity(predicted_class):
    import collections
    out_put = []
    for i in range(len(predicted_class)):
        if predicted_class[i] == 0:
            #out_put[int_features[i]].append(1)
            out_put.append('active')
        else:
            #out_put[int_features[i]].append(2)
            out_put.append('non-active')
    return out_put


@app.route('/analyze_peptide_sequence', methods=['POST'])
def analyze_peptide_sequence():
    # 每一个网页上的 输入的框，是一个单独的x，下面这个就是吧这个单独的信息变成一个list，每一个单独的就是一个str （也可以吧x变成int 如果想要的话）
    # int_features  = [str(x) for x in request.form.values()] # this command basically use extract all the input into a list
    #final_features = [np.array(int_features)]
    int_features  = [str(x) for x in request.form.values()]
    sequence_list=int_features[0].split(',')  # 因为这个list里面只有一个element，所以我只需要把吧这个拿出来，然后split
    # 另外需要注意，这个地方，网页上输入的时候必须要是AAA,CCC,SAS, 这个格式，不同的sequence的区分只能使用逗号，其他的都不可以
    peptide_sequence_list = []
    for seq in sequence_list:
        format_seq = [seq, seq]  # the setting is just following the input format setting in ESM model, [name,sequence]
        tuple_sequence = tuple(format_seq)
        peptide_sequence_list.append(tuple_sequence)  # build a summarize list variable including all the sequence information

    embeddings_results = esm_embeddings(peptide_sequence_list) #conduct the embedding
    normalized_embeddings_results = scaler.transform(embeddings_results) # normalized the embeddings

    # prediction
    predicted_protability = model.predict(normalized_embeddings_results, batch_size=1)
    predicted_class = []
    for i in range(predicted_protability.shape[0]):
        index = np.where(predicted_protability[i] == np.amax(predicted_protability[i]))[0][0]
        predicted_class.append(index) # get the class of the results
    predicted_class = assign_activity(predicted_class) # transform results (0 and 1) into 'active' and 'non-active'

    return render_template('peptide_activity_prediction.html',prediction_text="Number of Weekly Rides Should be {}".format(predicted_class))

#---------------------------++++++++++++++++++++++++++++++++++++++++++++++++++
@app.route('/protein_structure_predictions')
def protein_structure_prediction():
    return render_template('protein_structure_prediction.html')

@app.route('/analyze_peptide_csv', methods=['POST'])
def analyze_protein_csv():
    # Implement CSV analysis logic here
    result = {"message": "CSV file analysis result"}
    return render_template('protein_structure__prediction.html', result=result)

@app.route('/analyze_peptide_sequence', methods=['POST'])
def analyze_protein_sequence():
    # Implement sequence analysis logic here
    result = {"message": "Sequence analysis result"}
    return render_template('protein_structure_prediction.html', result=result)




@app.route('/generative_models', methods=['POST'])
def generative():
    # Get the file_path from the form data

    return render_template('generative_modeling.html')


@app.route('/ml_rlwhl_models', methods=['POST'])
def rlwhl():
    # Get the file_path from the form data

    return render_template('ml_rlwhl.html')

if __name__ == "__main__":
    app.run(debug=True)